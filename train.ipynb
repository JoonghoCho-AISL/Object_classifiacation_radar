{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 15:54:18.977596: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-20 15:54:19.119583: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-20 15:54:19.151893: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-20 15:54:19.917464: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64:\n",
      "2022-09-20 15:54:19.917543: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:/usr/local/cuda-11.6/lib64:\n",
      "2022-09-20 15:54:19.917548: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from models import decision_forests\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idxDict = {\n",
    "    'metal' : 0,\n",
    "    'paper' : 1,\n",
    "    'plastic' : 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metal = np.loadtxt('./data/metal.csv', delimiter = ',')\n",
    "paper = np.loadtxt('./data/paper.csv', delimiter = ',')\n",
    "plastic = np.loadtxt('./data/plastic.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(arr, label):\n",
    "    label_list = [label for j in range(arr.shape[0])]\n",
    "    label_list = np.array(label_list)\n",
    "    label_list = np.reshape(label_list, (arr.shape[0], 1))\n",
    "    # print(label_list.shape)\n",
    "    labeled_arr = np.concatenate((arr, label_list), axis = 1)\n",
    "    return labeled_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metal = add_label(metal, label2idxDict['metal'])\n",
    "paper = add_label(paper, label2idxDict['paper'])\n",
    "plastic = add_label(plastic, label2idxDict['plastic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 41)\n"
     ]
    }
   ],
   "source": [
    "print(plastic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((metal, paper, plastic), axis = 0)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data[:,:-1], data[:,-1], test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The `num_threads` constructor argument is not set and the number of CPU is os.cpu_count()=80 > 32. Setting num_threads to 32. Set num_threads manually to use more than 32 cpus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `num_threads` constructor argument is not set and the number of CPU is os.cpu_count()=80 > 32. Setting num_threads to 32. Set num_threads manually to use more than 32 cpus.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use /tmp/tmpdp4cwqx7 as temporary training directory\n"
     ]
    }
   ],
   "source": [
    "model = decision_forests()\n",
    "model.compile(\n",
    "    metrics=['accuray', 'loss']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training dataset...\n",
      "Training dataset read in 0:00:01.822678. Found 63 examples.\n",
      "Training model...\n",
      "Model trained in 0:00:00.047180\n",
      "Compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO kernel.cc:1176] Loading model from path /tmp/tmpdp4cwqx7/model/ with prefix 9df1c317fb724b60\n",
      "[INFO abstract_model.cc:1248] Engine \"RandomForestGeneric\" built\n",
      "[INFO kernel.cc:1022] Use fast generic engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7f1c3e63b4c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7f1c3e63b4c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7f1c3e63b4c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model compiled.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"random_forest_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      "=================================================================\n",
      "Total params: 1\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1\n",
      "_________________________________________________________________\n",
      "Type: \"RANDOM_FOREST\"\n",
      "Task: CLASSIFICATION\n",
      "Label: \"__LABEL\"\n",
      "\n",
      "Input Features (40):\n",
      "\tdata:0.0\n",
      "\tdata:0.1\n",
      "\tdata:0.10\n",
      "\tdata:0.11\n",
      "\tdata:0.12\n",
      "\tdata:0.13\n",
      "\tdata:0.14\n",
      "\tdata:0.15\n",
      "\tdata:0.16\n",
      "\tdata:0.17\n",
      "\tdata:0.18\n",
      "\tdata:0.19\n",
      "\tdata:0.2\n",
      "\tdata:0.20\n",
      "\tdata:0.21\n",
      "\tdata:0.22\n",
      "\tdata:0.23\n",
      "\tdata:0.24\n",
      "\tdata:0.25\n",
      "\tdata:0.26\n",
      "\tdata:0.27\n",
      "\tdata:0.28\n",
      "\tdata:0.29\n",
      "\tdata:0.3\n",
      "\tdata:0.30\n",
      "\tdata:0.31\n",
      "\tdata:0.32\n",
      "\tdata:0.33\n",
      "\tdata:0.34\n",
      "\tdata:0.35\n",
      "\tdata:0.36\n",
      "\tdata:0.37\n",
      "\tdata:0.38\n",
      "\tdata:0.39\n",
      "\tdata:0.4\n",
      "\tdata:0.5\n",
      "\tdata:0.6\n",
      "\tdata:0.7\n",
      "\tdata:0.8\n",
      "\tdata:0.9\n",
      "\n",
      "No weights\n",
      "\n",
      "Variable Importance: MEAN_MIN_DEPTH:\n",
      "    1. \"data:0.38\"  2.043833 ################\n",
      "    2.   \"__LABEL\"  2.043833 ################\n",
      "    3. \"data:0.34\"  2.042500 ###############\n",
      "    4. \"data:0.36\"  2.042167 ###############\n",
      "    5. \"data:0.17\"  2.042167 ###############\n",
      "    6. \"data:0.39\"  2.042167 ###############\n",
      "    7. \"data:0.37\"  2.040833 ###############\n",
      "    8. \"data:0.18\"  2.040500 ###############\n",
      "    9. \"data:0.32\"  2.040500 ###############\n",
      "   10. \"data:0.28\"  2.039500 ###############\n",
      "   11. \"data:0.27\"  2.039167 ###############\n",
      "   12. \"data:0.19\"  2.038833 ###############\n",
      "   13. \"data:0.31\"  2.037722 ###############\n",
      "   14. \"data:0.33\"  2.037500 ###############\n",
      "   15. \"data:0.35\"  2.037500 ###############\n",
      "   16. \"data:0.24\"  2.035500 ###############\n",
      "   17. \"data:0.25\"  2.033278 ###############\n",
      "   18. \"data:0.20\"  2.032833 ###############\n",
      "   19. \"data:0.30\"  2.030500 ###############\n",
      "   20. \"data:0.26\"  2.029500 ###############\n",
      "   21. \"data:0.29\"  2.028833 ###############\n",
      "   22. \"data:0.21\"  2.027833 ###############\n",
      "   23. \"data:0.15\"  2.025833 ###############\n",
      "   24. \"data:0.14\"  2.021167 ###############\n",
      "   25. \"data:0.16\"  2.020833 ###############\n",
      "   26. \"data:0.22\"  2.013389 ##############\n",
      "   27. \"data:0.13\"  2.006833 ##############\n",
      "   28. \"data:0.23\"  2.001111 ##############\n",
      "   29. \"data:0.12\"  1.961444 ############\n",
      "   30.  \"data:0.1\"  1.946944 ############\n",
      "   31. \"data:0.11\"  1.925167 ###########\n",
      "   32.  \"data:0.0\"  1.924000 ###########\n",
      "   33.  \"data:0.2\"  1.848833 ########\n",
      "   34. \"data:0.10\"  1.809667 ######\n",
      "   35.  \"data:0.3\"  1.802444 ######\n",
      "   36.  \"data:0.8\"  1.797889 ######\n",
      "   37.  \"data:0.4\"  1.785333 ######\n",
      "   38.  \"data:0.9\"  1.738111 ####\n",
      "   39.  \"data:0.6\"  1.728611 ###\n",
      "   40.  \"data:0.7\"  1.702833 ##\n",
      "   41.  \"data:0.5\"  1.629833 \n",
      "\n",
      "Variable Importance: NUM_AS_ROOT:\n",
      "    1.  \"data:0.5\" 42.000000 ################\n",
      "    2.  \"data:0.7\" 36.000000 #############\n",
      "    3.  \"data:0.6\" 32.000000 ############\n",
      "    4.  \"data:0.9\" 30.000000 ###########\n",
      "    5.  \"data:0.4\" 27.000000 ##########\n",
      "    6.  \"data:0.8\" 24.000000 ########\n",
      "    7. \"data:0.10\" 23.000000 ########\n",
      "    8.  \"data:0.3\" 20.000000 #######\n",
      "    9.  \"data:0.2\" 19.000000 #######\n",
      "   10.  \"data:0.0\" 10.000000 ###\n",
      "   11. \"data:0.11\" 10.000000 ###\n",
      "   12. \"data:0.12\"  9.000000 ###\n",
      "   13.  \"data:0.1\"  8.000000 ##\n",
      "   14. \"data:0.13\"  3.000000 \n",
      "   15. \"data:0.14\"  2.000000 \n",
      "   16. \"data:0.15\"  2.000000 \n",
      "   17. \"data:0.16\"  2.000000 \n",
      "   18. \"data:0.25\"  1.000000 \n",
      "\n",
      "Variable Importance: NUM_NODES:\n",
      "    1.  \"data:0.5\" 91.000000 ################\n",
      "    2.  \"data:0.6\" 79.000000 #############\n",
      "    3.  \"data:0.7\" 75.000000 #############\n",
      "    4.  \"data:0.9\" 69.000000 ############\n",
      "    5.  \"data:0.3\" 60.000000 ##########\n",
      "    6.  \"data:0.8\" 60.000000 ##########\n",
      "    7.  \"data:0.4\" 59.000000 ##########\n",
      "    8. \"data:0.10\" 54.000000 #########\n",
      "    9.  \"data:0.2\" 51.000000 ########\n",
      "   10.  \"data:0.0\" 28.000000 ####\n",
      "   11.  \"data:0.1\" 24.000000 ####\n",
      "   12. \"data:0.11\" 24.000000 ####\n",
      "   13. \"data:0.23\" 21.000000 ###\n",
      "   14. \"data:0.12\" 16.000000 ##\n",
      "   15. \"data:0.22\" 16.000000 ##\n",
      "   16. \"data:0.21\" 10.000000 #\n",
      "   17. \"data:0.26\"  9.000000 #\n",
      "   18. \"data:0.30\"  8.000000 #\n",
      "   19. \"data:0.14\"  7.000000 #\n",
      "   20. \"data:0.20\"  7.000000 #\n",
      "   21. \"data:0.29\"  7.000000 #\n",
      "   22. \"data:0.13\"  6.000000 \n",
      "   23. \"data:0.16\"  6.000000 \n",
      "   24. \"data:0.24\"  5.000000 \n",
      "   25. \"data:0.15\"  4.000000 \n",
      "   26. \"data:0.31\"  4.000000 \n",
      "   27. \"data:0.33\"  4.000000 \n",
      "   28. \"data:0.35\"  4.000000 \n",
      "   29. \"data:0.19\"  3.000000 \n",
      "   30. \"data:0.27\"  3.000000 \n",
      "   31. \"data:0.28\"  3.000000 \n",
      "   32. \"data:0.18\"  2.000000 \n",
      "   33. \"data:0.25\"  2.000000 \n",
      "   34. \"data:0.32\"  2.000000 \n",
      "   35. \"data:0.37\"  2.000000 \n",
      "   36. \"data:0.17\"  1.000000 \n",
      "   37. \"data:0.34\"  1.000000 \n",
      "   38. \"data:0.36\"  1.000000 \n",
      "   39. \"data:0.39\"  1.000000 \n",
      "\n",
      "Variable Importance: SUM_SCORE:\n",
      "    1.  \"data:0.5\" 2674.498699 ################\n",
      "    2.  \"data:0.6\" 2207.932422 #############\n",
      "    3.  \"data:0.7\" 2187.127719 #############\n",
      "    4.  \"data:0.9\" 2024.994651 ############\n",
      "    5.  \"data:0.4\" 1689.215639 ##########\n",
      "    6.  \"data:0.8\" 1563.307580 #########\n",
      "    7.  \"data:0.3\" 1447.059647 ########\n",
      "    8. \"data:0.10\" 1377.917583 ########\n",
      "    9.  \"data:0.2\" 1225.538073 #######\n",
      "   10.  \"data:0.0\" 680.606251 ####\n",
      "   11. \"data:0.11\" 600.313612 ###\n",
      "   12.  \"data:0.1\" 574.286932 ###\n",
      "   13. \"data:0.12\" 363.061671 ##\n",
      "   14. \"data:0.23\" 160.120412 \n",
      "   15. \"data:0.13\" 139.696543 \n",
      "   16. \"data:0.22\" 103.163057 \n",
      "   17. \"data:0.14\" 69.060593 \n",
      "   18. \"data:0.16\" 66.192083 \n",
      "   19. \"data:0.15\" 64.827913 \n",
      "   20. \"data:0.26\" 42.801648 \n",
      "   21. \"data:0.21\" 21.859134 \n",
      "   22. \"data:0.30\" 20.545926 \n",
      "   23. \"data:0.25\" 16.962036 \n",
      "   24. \"data:0.29\" 16.905324 \n",
      "   25. \"data:0.20\" 16.237485 \n",
      "   26. \"data:0.31\" 12.354877 \n",
      "   27. \"data:0.19\" 11.063674 \n",
      "   28. \"data:0.24\" 10.756238 \n",
      "   29. \"data:0.18\" 10.507679 \n",
      "   30. \"data:0.33\"  9.559357 \n",
      "   31. \"data:0.27\"  8.643870 \n",
      "   32. \"data:0.35\"  7.900029 \n",
      "   33. \"data:0.32\"  4.744715 \n",
      "   34. \"data:0.28\"  4.153184 \n",
      "   35. \"data:0.39\"  3.136601 \n",
      "   36. \"data:0.37\"  2.558697 \n",
      "   37. \"data:0.17\"  1.847841 \n",
      "   38. \"data:0.34\"  1.360053 \n",
      "   39. \"data:0.36\"  1.100449 \n",
      "\n",
      "\n",
      "\n",
      "Winner take all: true\n",
      "Out-of-bag evaluation: accuracy:0.984127 logloss:0.646281\n",
      "Number of trees: 300\n",
      "Total number of nodes: 1958\n",
      "\n",
      "Number of nodes by tree:\n",
      "Count: 300 Average: 6.52667 StdDev: 1.21214\n",
      "Min: 5 Max: 11 Ignored: 0\n",
      "----------------------------------------------\n",
      "[  5,  6)  98  32.67%  32.67% ######\n",
      "[  6,  7)   0   0.00%  32.67%\n",
      "[  7,  8) 176  58.67%  91.33% ##########\n",
      "[  8,  9)   0   0.00%  91.33%\n",
      "[  9, 10)  25   8.33%  99.67% #\n",
      "[ 10, 11)   0   0.00%  99.67%\n",
      "[ 11, 11]   1   0.33% 100.00%\n",
      "\n",
      "Depth by leafs:\n",
      "Count: 1129 Average: 2.0868 StdDev: 0.711473\n",
      "Min: 1 Max: 4 Ignored: 0\n",
      "----------------------------------------------\n",
      "[ 1, 2) 239  21.17%  21.17% ####\n",
      "[ 2, 3) 555  49.16%  70.33% ##########\n",
      "[ 3, 4) 333  29.50%  99.82% ######\n",
      "[ 4, 4]   2   0.18% 100.00%\n",
      "\n",
      "Number of training obs by leaf:\n",
      "Count: 1129 Average: 16.7405 StdDev: 7.3655\n",
      "Min: 5 Max: 34 Ignored: 0\n",
      "----------------------------------------------\n",
      "[  5,  6) 185  16.39%  16.39% ##########\n",
      "[  6,  8)  40   3.54%  19.93% ##\n",
      "[  8,  9)   9   0.80%  20.73%\n",
      "[  9, 11)  28   2.48%  23.21% ##\n",
      "[ 11, 12)  20   1.77%  24.98% #\n",
      "[ 12, 14)  67   5.93%  30.91% ####\n",
      "[ 14, 15)  36   3.19%  34.10% ##\n",
      "[ 15, 17)  99   8.77%  42.87% #####\n",
      "[ 17, 18)  60   5.31%  48.18% ###\n",
      "[ 18, 20) 111   9.83%  58.02% ######\n",
      "[ 20, 21)  71   6.29%  64.30% ####\n",
      "[ 21, 23) 125  11.07%  75.38% #######\n",
      "[ 23, 24)  58   5.14%  80.51% ###\n",
      "[ 24, 26) 102   9.03%  89.55% ######\n",
      "[ 26, 27)  39   3.45%  93.00% ##\n",
      "[ 27, 29)  48   4.25%  97.25% ###\n",
      "[ 29, 30)   9   0.80%  98.05%\n",
      "[ 30, 32)  19   1.68%  99.73% #\n",
      "[ 32, 33)   1   0.09%  99.82%\n",
      "[ 33, 34]   2   0.18% 100.00%\n",
      "\n",
      "Attribute in nodes:\n",
      "\t91 : data:0.5 [NUMERICAL]\n",
      "\t79 : data:0.6 [NUMERICAL]\n",
      "\t75 : data:0.7 [NUMERICAL]\n",
      "\t69 : data:0.9 [NUMERICAL]\n",
      "\t60 : data:0.8 [NUMERICAL]\n",
      "\t60 : data:0.3 [NUMERICAL]\n",
      "\t59 : data:0.4 [NUMERICAL]\n",
      "\t54 : data:0.10 [NUMERICAL]\n",
      "\t51 : data:0.2 [NUMERICAL]\n",
      "\t28 : data:0.0 [NUMERICAL]\n",
      "\t24 : data:0.11 [NUMERICAL]\n",
      "\t24 : data:0.1 [NUMERICAL]\n",
      "\t21 : data:0.23 [NUMERICAL]\n",
      "\t16 : data:0.22 [NUMERICAL]\n",
      "\t16 : data:0.12 [NUMERICAL]\n",
      "\t10 : data:0.21 [NUMERICAL]\n",
      "\t9 : data:0.26 [NUMERICAL]\n",
      "\t8 : data:0.30 [NUMERICAL]\n",
      "\t7 : data:0.29 [NUMERICAL]\n",
      "\t7 : data:0.20 [NUMERICAL]\n",
      "\t7 : data:0.14 [NUMERICAL]\n",
      "\t6 : data:0.16 [NUMERICAL]\n",
      "\t6 : data:0.13 [NUMERICAL]\n",
      "\t5 : data:0.24 [NUMERICAL]\n",
      "\t4 : data:0.35 [NUMERICAL]\n",
      "\t4 : data:0.33 [NUMERICAL]\n",
      "\t4 : data:0.31 [NUMERICAL]\n",
      "\t4 : data:0.15 [NUMERICAL]\n",
      "\t3 : data:0.28 [NUMERICAL]\n",
      "\t3 : data:0.27 [NUMERICAL]\n",
      "\t3 : data:0.19 [NUMERICAL]\n",
      "\t2 : data:0.37 [NUMERICAL]\n",
      "\t2 : data:0.32 [NUMERICAL]\n",
      "\t2 : data:0.25 [NUMERICAL]\n",
      "\t2 : data:0.18 [NUMERICAL]\n",
      "\t1 : data:0.39 [NUMERICAL]\n",
      "\t1 : data:0.36 [NUMERICAL]\n",
      "\t1 : data:0.34 [NUMERICAL]\n",
      "\t1 : data:0.17 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 0:\n",
      "\t42 : data:0.5 [NUMERICAL]\n",
      "\t36 : data:0.7 [NUMERICAL]\n",
      "\t32 : data:0.6 [NUMERICAL]\n",
      "\t30 : data:0.9 [NUMERICAL]\n",
      "\t27 : data:0.4 [NUMERICAL]\n",
      "\t24 : data:0.8 [NUMERICAL]\n",
      "\t23 : data:0.10 [NUMERICAL]\n",
      "\t20 : data:0.3 [NUMERICAL]\n",
      "\t19 : data:0.2 [NUMERICAL]\n",
      "\t10 : data:0.11 [NUMERICAL]\n",
      "\t10 : data:0.0 [NUMERICAL]\n",
      "\t9 : data:0.12 [NUMERICAL]\n",
      "\t8 : data:0.1 [NUMERICAL]\n",
      "\t3 : data:0.13 [NUMERICAL]\n",
      "\t2 : data:0.16 [NUMERICAL]\n",
      "\t2 : data:0.15 [NUMERICAL]\n",
      "\t2 : data:0.14 [NUMERICAL]\n",
      "\t1 : data:0.25 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 1:\n",
      "\t82 : data:0.5 [NUMERICAL]\n",
      "\t70 : data:0.6 [NUMERICAL]\n",
      "\t68 : data:0.7 [NUMERICAL]\n",
      "\t65 : data:0.9 [NUMERICAL]\n",
      "\t54 : data:0.4 [NUMERICAL]\n",
      "\t53 : data:0.8 [NUMERICAL]\n",
      "\t51 : data:0.3 [NUMERICAL]\n",
      "\t46 : data:0.10 [NUMERICAL]\n",
      "\t42 : data:0.2 [NUMERICAL]\n",
      "\t27 : data:0.0 [NUMERICAL]\n",
      "\t24 : data:0.11 [NUMERICAL]\n",
      "\t20 : data:0.1 [NUMERICAL]\n",
      "\t15 : data:0.12 [NUMERICAL]\n",
      "\t8 : data:0.23 [NUMERICAL]\n",
      "\t6 : data:0.13 [NUMERICAL]\n",
      "\t5 : data:0.22 [NUMERICAL]\n",
      "\t5 : data:0.16 [NUMERICAL]\n",
      "\t3 : data:0.15 [NUMERICAL]\n",
      "\t3 : data:0.14 [NUMERICAL]\n",
      "\t2 : data:0.26 [NUMERICAL]\n",
      "\t2 : data:0.21 [NUMERICAL]\n",
      "\t2 : data:0.19 [NUMERICAL]\n",
      "\t1 : data:0.35 [NUMERICAL]\n",
      "\t1 : data:0.33 [NUMERICAL]\n",
      "\t1 : data:0.32 [NUMERICAL]\n",
      "\t1 : data:0.29 [NUMERICAL]\n",
      "\t1 : data:0.28 [NUMERICAL]\n",
      "\t1 : data:0.27 [NUMERICAL]\n",
      "\t1 : data:0.25 [NUMERICAL]\n",
      "\t1 : data:0.17 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 2:\n",
      "\t91 : data:0.5 [NUMERICAL]\n",
      "\t79 : data:0.6 [NUMERICAL]\n",
      "\t75 : data:0.7 [NUMERICAL]\n",
      "\t69 : data:0.9 [NUMERICAL]\n",
      "\t60 : data:0.8 [NUMERICAL]\n",
      "\t60 : data:0.3 [NUMERICAL]\n",
      "\t59 : data:0.4 [NUMERICAL]\n",
      "\t54 : data:0.10 [NUMERICAL]\n",
      "\t51 : data:0.2 [NUMERICAL]\n",
      "\t28 : data:0.0 [NUMERICAL]\n",
      "\t24 : data:0.11 [NUMERICAL]\n",
      "\t23 : data:0.1 [NUMERICAL]\n",
      "\t21 : data:0.23 [NUMERICAL]\n",
      "\t16 : data:0.22 [NUMERICAL]\n",
      "\t16 : data:0.12 [NUMERICAL]\n",
      "\t10 : data:0.21 [NUMERICAL]\n",
      "\t9 : data:0.26 [NUMERICAL]\n",
      "\t8 : data:0.30 [NUMERICAL]\n",
      "\t7 : data:0.29 [NUMERICAL]\n",
      "\t7 : data:0.20 [NUMERICAL]\n",
      "\t7 : data:0.14 [NUMERICAL]\n",
      "\t6 : data:0.16 [NUMERICAL]\n",
      "\t6 : data:0.13 [NUMERICAL]\n",
      "\t5 : data:0.24 [NUMERICAL]\n",
      "\t4 : data:0.35 [NUMERICAL]\n",
      "\t4 : data:0.33 [NUMERICAL]\n",
      "\t4 : data:0.31 [NUMERICAL]\n",
      "\t4 : data:0.15 [NUMERICAL]\n",
      "\t3 : data:0.28 [NUMERICAL]\n",
      "\t3 : data:0.27 [NUMERICAL]\n",
      "\t3 : data:0.19 [NUMERICAL]\n",
      "\t2 : data:0.37 [NUMERICAL]\n",
      "\t2 : data:0.32 [NUMERICAL]\n",
      "\t2 : data:0.25 [NUMERICAL]\n",
      "\t2 : data:0.18 [NUMERICAL]\n",
      "\t1 : data:0.39 [NUMERICAL]\n",
      "\t1 : data:0.36 [NUMERICAL]\n",
      "\t1 : data:0.34 [NUMERICAL]\n",
      "\t1 : data:0.17 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 3:\n",
      "\t91 : data:0.5 [NUMERICAL]\n",
      "\t79 : data:0.6 [NUMERICAL]\n",
      "\t75 : data:0.7 [NUMERICAL]\n",
      "\t69 : data:0.9 [NUMERICAL]\n",
      "\t60 : data:0.8 [NUMERICAL]\n",
      "\t60 : data:0.3 [NUMERICAL]\n",
      "\t59 : data:0.4 [NUMERICAL]\n",
      "\t54 : data:0.10 [NUMERICAL]\n",
      "\t51 : data:0.2 [NUMERICAL]\n",
      "\t28 : data:0.0 [NUMERICAL]\n",
      "\t24 : data:0.11 [NUMERICAL]\n",
      "\t24 : data:0.1 [NUMERICAL]\n",
      "\t21 : data:0.23 [NUMERICAL]\n",
      "\t16 : data:0.22 [NUMERICAL]\n",
      "\t16 : data:0.12 [NUMERICAL]\n",
      "\t10 : data:0.21 [NUMERICAL]\n",
      "\t9 : data:0.26 [NUMERICAL]\n",
      "\t8 : data:0.30 [NUMERICAL]\n",
      "\t7 : data:0.29 [NUMERICAL]\n",
      "\t7 : data:0.20 [NUMERICAL]\n",
      "\t7 : data:0.14 [NUMERICAL]\n",
      "\t6 : data:0.16 [NUMERICAL]\n",
      "\t6 : data:0.13 [NUMERICAL]\n",
      "\t5 : data:0.24 [NUMERICAL]\n",
      "\t4 : data:0.35 [NUMERICAL]\n",
      "\t4 : data:0.33 [NUMERICAL]\n",
      "\t4 : data:0.31 [NUMERICAL]\n",
      "\t4 : data:0.15 [NUMERICAL]\n",
      "\t3 : data:0.28 [NUMERICAL]\n",
      "\t3 : data:0.27 [NUMERICAL]\n",
      "\t3 : data:0.19 [NUMERICAL]\n",
      "\t2 : data:0.37 [NUMERICAL]\n",
      "\t2 : data:0.32 [NUMERICAL]\n",
      "\t2 : data:0.25 [NUMERICAL]\n",
      "\t2 : data:0.18 [NUMERICAL]\n",
      "\t1 : data:0.39 [NUMERICAL]\n",
      "\t1 : data:0.36 [NUMERICAL]\n",
      "\t1 : data:0.34 [NUMERICAL]\n",
      "\t1 : data:0.17 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 5:\n",
      "\t91 : data:0.5 [NUMERICAL]\n",
      "\t79 : data:0.6 [NUMERICAL]\n",
      "\t75 : data:0.7 [NUMERICAL]\n",
      "\t69 : data:0.9 [NUMERICAL]\n",
      "\t60 : data:0.8 [NUMERICAL]\n",
      "\t60 : data:0.3 [NUMERICAL]\n",
      "\t59 : data:0.4 [NUMERICAL]\n",
      "\t54 : data:0.10 [NUMERICAL]\n",
      "\t51 : data:0.2 [NUMERICAL]\n",
      "\t28 : data:0.0 [NUMERICAL]\n",
      "\t24 : data:0.11 [NUMERICAL]\n",
      "\t24 : data:0.1 [NUMERICAL]\n",
      "\t21 : data:0.23 [NUMERICAL]\n",
      "\t16 : data:0.22 [NUMERICAL]\n",
      "\t16 : data:0.12 [NUMERICAL]\n",
      "\t10 : data:0.21 [NUMERICAL]\n",
      "\t9 : data:0.26 [NUMERICAL]\n",
      "\t8 : data:0.30 [NUMERICAL]\n",
      "\t7 : data:0.29 [NUMERICAL]\n",
      "\t7 : data:0.20 [NUMERICAL]\n",
      "\t7 : data:0.14 [NUMERICAL]\n",
      "\t6 : data:0.16 [NUMERICAL]\n",
      "\t6 : data:0.13 [NUMERICAL]\n",
      "\t5 : data:0.24 [NUMERICAL]\n",
      "\t4 : data:0.35 [NUMERICAL]\n",
      "\t4 : data:0.33 [NUMERICAL]\n",
      "\t4 : data:0.31 [NUMERICAL]\n",
      "\t4 : data:0.15 [NUMERICAL]\n",
      "\t3 : data:0.28 [NUMERICAL]\n",
      "\t3 : data:0.27 [NUMERICAL]\n",
      "\t3 : data:0.19 [NUMERICAL]\n",
      "\t2 : data:0.37 [NUMERICAL]\n",
      "\t2 : data:0.32 [NUMERICAL]\n",
      "\t2 : data:0.25 [NUMERICAL]\n",
      "\t2 : data:0.18 [NUMERICAL]\n",
      "\t1 : data:0.39 [NUMERICAL]\n",
      "\t1 : data:0.36 [NUMERICAL]\n",
      "\t1 : data:0.34 [NUMERICAL]\n",
      "\t1 : data:0.17 [NUMERICAL]\n",
      "\n",
      "Condition type in nodes:\n",
      "\t829 : HigherCondition\n",
      "Condition type in nodes with depth <= 0:\n",
      "\t300 : HigherCondition\n",
      "Condition type in nodes with depth <= 1:\n",
      "\t661 : HigherCondition\n",
      "Condition type in nodes with depth <= 2:\n",
      "\t828 : HigherCondition\n",
      "Condition type in nodes with depth <= 3:\n",
      "\t829 : HigherCondition\n",
      "Condition type in nodes with depth <= 5:\n",
      "\t829 : HigherCondition\n",
      "Node format: NOT_SET\n",
      "\n",
      "Training OOB:\n",
      "\ttrees: 1, Out-of-bag evaluation: accuracy:0.863636 logloss:4.91504\n",
      "\ttrees: 12, Out-of-bag evaluation: accuracy:0.968254 logloss:1.18157\n",
      "\ttrees: 22, Out-of-bag evaluation: accuracy:0.968254 logloss:0.660106\n",
      "\ttrees: 32, Out-of-bag evaluation: accuracy:0.952381 logloss:0.654621\n",
      "\ttrees: 42, Out-of-bag evaluation: accuracy:0.936508 logloss:0.658296\n",
      "\ttrees: 52, Out-of-bag evaluation: accuracy:0.936508 logloss:0.659712\n",
      "\ttrees: 63, Out-of-bag evaluation: accuracy:0.952381 logloss:0.65145\n",
      "\ttrees: 73, Out-of-bag evaluation: accuracy:0.968254 logloss:0.644113\n",
      "\ttrees: 83, Out-of-bag evaluation: accuracy:0.984127 logloss:0.642589\n",
      "\ttrees: 93, Out-of-bag evaluation: accuracy:0.984127 logloss:0.64061\n",
      "\ttrees: 103, Out-of-bag evaluation: accuracy:0.984127 logloss:0.642153\n",
      "\ttrees: 113, Out-of-bag evaluation: accuracy:0.984127 logloss:0.643527\n",
      "\ttrees: 124, Out-of-bag evaluation: accuracy:0.984127 logloss:0.641396\n",
      "\ttrees: 134, Out-of-bag evaluation: accuracy:0.984127 logloss:0.642585\n",
      "\ttrees: 144, Out-of-bag evaluation: accuracy:0.984127 logloss:0.642117\n",
      "\ttrees: 154, Out-of-bag evaluation: accuracy:0.984127 logloss:0.64218\n",
      "\ttrees: 164, Out-of-bag evaluation: accuracy:0.984127 logloss:0.64154\n",
      "\ttrees: 175, Out-of-bag evaluation: accuracy:0.984127 logloss:0.642218\n",
      "\ttrees: 185, Out-of-bag evaluation: accuracy:0.984127 logloss:0.640267\n",
      "\ttrees: 195, Out-of-bag evaluation: accuracy:0.984127 logloss:0.642038\n",
      "\ttrees: 205, Out-of-bag evaluation: accuracy:0.984127 logloss:0.644339\n",
      "\ttrees: 216, Out-of-bag evaluation: accuracy:0.984127 logloss:0.644189\n",
      "\ttrees: 226, Out-of-bag evaluation: accuracy:0.984127 logloss:0.646012\n",
      "\ttrees: 237, Out-of-bag evaluation: accuracy:0.984127 logloss:0.646415\n",
      "\ttrees: 248, Out-of-bag evaluation: accuracy:0.984127 logloss:0.646408\n",
      "\ttrees: 258, Out-of-bag evaluation: accuracy:0.984127 logloss:0.646308\n",
      "\ttrees: 268, Out-of-bag evaluation: accuracy:0.984127 logloss:0.644826\n",
      "\ttrees: 278, Out-of-bag evaluation: accuracy:0.984127 logloss:0.645364\n",
      "\ttrees: 289, Out-of-bag evaluation: accuracy:0.984127 logloss:0.645536\n",
      "\ttrees: 299, Out-of-bag evaluation: accuracy:0.984127 logloss:0.646434\n",
      "\ttrees: 300, Out-of-bag evaluation: accuracy:0.984127 logloss:0.646281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(num_examples=63, accuracy=0.9841269841269841, loss=0.6462813568198018, rmse=None, ndcg=None, aucs=None, auuc=None, qini=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.make_inspector().evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as call_get_leaves while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/RF/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/RF/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model/RF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel/RF.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/121/lib/python3.8/site-packages/tensorflow_decision_forests/keras/core.py:1671\u001b[0m, in \u001b[0;36mCoreModel.save\u001b[0;34m(self, filepath, overwrite, **kwargs)\u001b[0m\n\u001b[1;32m   1666\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1667\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1668\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mA model already exist as \u001b[39m\u001b[39m{\u001b[39;00mfilepath\u001b[39m}\u001b[39;00m\u001b[39m. Use an empty directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1669\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor set overwrite=True\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1671\u001b[0m \u001b[39msuper\u001b[39;49m(CoreModel, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49msave(\n\u001b[1;32m   1672\u001b[0m     filepath\u001b[39m=\u001b[39;49mfilepath, overwrite\u001b[39m=\u001b[39;49moverwrite, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/121/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/121/lib/python3.8/site-packages/keras/saving/save.py:153\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    145\u001b[0m     save_format \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mh5\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     \u001b[39mor\u001b[39;00m (h5py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath, h5py\u001b[39m.\u001b[39mFile))\n\u001b[1;32m    147\u001b[0m     \u001b[39mor\u001b[39;00m saving_utils\u001b[39m.\u001b[39mis_hdf5_filepath(filepath)\n\u001b[1;32m    148\u001b[0m ):\n\u001b[1;32m    149\u001b[0m     \u001b[39m# TODO(b/130258301): add utility method for detecting model type.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39m_is_graph_network \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    151\u001b[0m         model, sequential\u001b[39m.\u001b[39mSequential\n\u001b[1;32m    152\u001b[0m     ):\n\u001b[0;32m--> 153\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    154\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSaving the model to HDF5 format requires the model to be a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFunctional model or a Sequential model. It does not work for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msubclassed models, because such models are defined via the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mbody of a Python method, which isn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt safely serializable. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mConsider saving to the Tensorflow SavedModel format (by \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m             \u001b[39m'\u001b[39m\u001b[39msetting save_format=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m) or using `save_weights`.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    160\u001b[0m         )\n\u001b[1;32m    161\u001b[0m     hdf5_format\u001b[39m.\u001b[39msave_model_to_hdf5(\n\u001b[1;32m    162\u001b[0m         model, filepath, overwrite, include_optimizer\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."
     ]
    }
   ],
   "source": [
    "model.save('model/RF.h5',save_format=\"h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('121')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f653e4f81966997bb2ee7c4a98d04a7987c679ce00b753cdfa7b1b174b15344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
